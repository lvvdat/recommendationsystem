def process_text(text, emoji_dict, teen_dict, wrong_lst,english_lst):
    from underthesea import word_tokenize, pos_tag, sent_tokenize
    import regex
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        sentence = ''.join(english_lst[word]+' ' if word in english_lst else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))
        ###### DEL wrong words   
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        ###### DEL  words 
        sentence = ' '.join('' if len(word)>7  else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '                    
    document = new_sentence   
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    #...
    return document 
def process_emoji(text):
    from underthesea import word_tokenize, pos_tag, sent_tokenize
    import regex
    kk=['â€”', 'l', 'â€¦', 'â€¢', ';', 'â€“', 'k', 'c', 'â€œ', 'â€', 'a', 'â–²',
       '\uf043', 'á»§', 'b', 'm', 'd', 'Ä‘', '_', '|', 'v', 'âž¦', '?', 'âœ§',
       'â–º', 'e', 'Ã´', 'â˜…', 'Ì£', 'ðš', 'Ì‰', 'Ì€', 'Ì›', 'ð¢', 'Ìƒ', 'ð¨', 'Ì‚',
       'ð§', 'j', 'y', 't', 'o', 'â†’', 'âœš', 'h', 'u', 'â™¡', 'x', 'âœ¦', 'âœª',
       'w', 'â˜ž', 'â†', 'ã€', 'ã€‘', 'ð—”', 'Ì', 'r', 'â—', 'Â¬', 'p', 'q', '$',
       '@', 'Â¦', 'n', 'Â·', 'â– ', 's', 'z', 'â¥', 'Ì†', 'ð—°', 'ð—½', 'ð—»', 'ð—¶',
       'ð—®', 'ð—¢', 'ð—¦', 'ð— ', 'ð—Ÿ', 'ã€', "'", 'ð‘¨', 'ð’', 'ï¼', '\uf0a7', 'â˜†',
       'Â°', '\\', '\u202a', 'â—‹', 'f', 'â‚«', 'â—¤', 'â—†', 'â”ˆ', 'ÍŸ', 'g', 'áº«',
       'áº·', '\u200d', 'á»©', 'ï¿½', '\uf0d6', '\uf0fc', '\uf0d8', 'ï¼š', 'ð‘œ',
       'ð‘›', 'ð‘–', 'ð‘Ž', 'Ã©', 'ð—¼', 'á»‘', 'â€»', 'Â±', 'â•”', 'â•', 'â•—', 'â•š', 'â•',
       'ð‹', 'ð', '{', '}', 'âœ©', 'Ã²', 'âœ', 'ï¼Œ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â–',
       '\x08', 'Â´', 'Ã­', 'Â¥', 'âž¤', 'â¦', 'â™¤', 'â˜›', 'â€', 'ï¼ˆ', 'ï¼‰', 'ðŒ', 'â€˜',
       'â€™', 'â–¼', '\u202c', 'ð—º', 'ð˜‚', 'ð˜', 'ð—–', 'ð—œ', 'ð’–', 'ð’‘', 'ð’š', 'ð’',
       'ð’Š', 'ð’•', 'âœ¿', 'âœ“', 'á»‰', 'ï¿¼', 'ð™©', 'ð­', 'ð²', 'ð™¤', 'ð™£', 'âƒ£', 'â–½',
       'ç©¿', 'ð“·', 'ð“¶', 'ð®', 'ðŽ', 'ð“', 'ð©', 'ð¦', 'ðœ', 'â„…', 'â—”', 'ð–™', 'ð–š',
       'Ò‰', 'â˜', 'ï¼›', 'ï¿¥', 'âˆš', 'â—‰', 'ï½ž', 'á»•', 'ð˜†', 'ð—§', 'ð—¸', 'â‘¶', 'â‘·',
       'ð™ž', 'ð™ª', 'ð™¢', 'ðˆ', '`', 'Ã¨', 'á»±', 'â—™', 'â˜¼', 'âž£', 'ð™', 'ð‘¶', 'ðŸ‡¯',
       'ðŸ‡ª', 'ðŸ‡¦', 'ðŸ‡³', 'ð–”', 'ð–ˆ', 'ð–Ž', 'ð–“', 'â‰¤', 'â—‘', '\uf0b7', 'Â»', 'Ã¢',
       'â“›', 'â“˜', 'â“ž', 'â“', 'â“œ', 'â“”', 'â“¢', 'â“—', 'â“Ÿ', 'âž«', 'âœ¸', '\u2063',
       'ð˜', 'ð“¸', 'ð“½', 'Ì¶', 'áº½', 'á»', 'áº§', 'á»', 'Ä©', 'á»ƒ', 'á»‹', 'á»™', 'áº¯',
       'Í¡', 'à²¥', 'Íœ', 'Ê–', 'Ãª', 'ð™˜', 'âžŠ', 'âž‹', 'âžŒ', 'âž', 'âžŽ', 'âž¬',
       '\uf0e8', 'ÖŽ', 'ð—ž', 'ð‘', 'ð‘¡', 'ð‘¢', 'ð‘š', 'ð—¨', 'âœ²', 'âžœ', 'âœ¬', 'Ã¬',
       'áº­', 'áº¹', 'áº¥', 'âž¥', 'ð’‚', 'âœ¯', 'áº£', 'ð’„', 'ð’Ž', 'ã€‚', '\uf076', 'Ã—',
       'áº»', '\x7f', 'Ã·', 'âˆ†', 'ð“¬', 'â‘ ', 'â—', 'ð‚', 'âœ®', 'â—¯', 'Â¸', 'Â¯', 'â—•',
       'â€¿', 'â', 'â¦', 'â–·', 'Ã¸', 'ð‘»', 'ð‘´', 'ð‘µ', 'Â½', 'âˆ—', '\uf0f0', 'â‡',
       'ðŸ‡°', 'ðŸ‡§', 'ðŸ‡´', 'ðŸ‡¾', 'âˆ', '\uf04f', 'ê§', 'à¼º', 'à¼’', 'à¼»', 'ê§‚', 'â”Š',
       '\u200c', 'ï¼', 'ð™°', 'ðš˜', 'ðš’', 'ðš¢', 'ðš—', 'ðšŠ', 'ðšŒ', 'ðš™', 'ðš', 'ðš–',
       'ðšž', 'â‰', 'â €', 'ãƒ»', 'â‰§', 'Ã¶', 'Ã±', 'â—', 'Ì„', 'Ð°', 'â€²', 'âŒ„', 'âœ¼',
       'Ï†', 'â–¹', 'âˆ©', 'ã¥', 'â•­', 'â—ˆ', 'â™›', '\uf06e', 'âƒ', 'á´€', 'â€•', 'â‡¨',
       'â‡¦', 'ð„', 'á»', 'ãƒŸ', 'å½¡', 'âž»', 'â”', 'â‡’', 'á»£', 'ð™–', 'ð’”', 'â…', 'âœ«',
       'âœ­', 'âœ°', 'âœ¹', 'âœ·', 'âœ¶', 'âœµ', 'âœ±', 'âŠ', 'âœ¾', 'âœ½', 'âœ ', 'âœº', 'â‹',
       'á»¥', 'â˜Ÿ', '\uf055', '\uf0ab', 'ðŠ', '\uf04a', 'âœ»', 'á»‡', 'â‚©', 'ï½¡',
       'ð', 'ð¬', 'ð‘ ', 'âˆž', 'Ãº', 'áº©', 'ð•”', 'ð•¦', 'ï¾‰', 'ï¾Ÿ', 'âˆ€', 'Ï‰', 'â—Œ',
       'â‘…', 'âƒ', 'â‹†', 'áƒ¦', 'â—¢', 'áº¿', 'ð€', 'á´—', 'ãƒ¾', 'â•¹', 'â—¡', 'ãƒŽ', 'ã€Š',
       'ã€‹', 'â˜œ', 'ã®', 'Ã³', 'â›¦', 'Ã£', 'á»—', 'Ë˜', 'Â³', 'Îµ', 'ï½€', 'ï¼¾', 'â„ƒ',
       'å£«', 'ï¹‹', 'ï¹Š', 'â™ª', 'â”…', 'è‹“', 'ï¼œ', 'â˜Š', 'â—¦', 'Â¡', 'ð¤', 'â™˜', 'è¥¦',
       'â‰¦', 'â—', 'Ëƒ', 'ðŸ‡«', 'Äƒ', 'ð™®', 'â¾', 'â·', 'ê™°', 'â€¡', 'Ëš', '\uf0e0',
       'âž¢', '\u202d', 'âš˜', 'ãŽ', 'á´œ', 'âžª', 'ã€Œ', 'ã€', 'ð˜µ', 'ð˜¢', 'ð˜ª', 'ð˜¶',
       'ð˜®', 'ð˜¯', 'â˜¬', 'â–ˆ', 'à®¿', 'ðŸ¡†', 'Ûž', 'à®', 'Å©', 'ð’†', 'ð‘', 'ð‘‡', 'Âº',
       'â™”', 'Î´', 'å¥³', 'ð™¥', 'â„‰', 'â¶', 'â¸', 'â¹', 'âº', 'ð’‰', 'ð’…', 'ð’ˆ', 'ð’Œ',
       'ð’“', 'ð‘²', 'ð˜¤', 'È¶', 'âœ†', 'ð±', 'ð', 'ð—µ', 'ð˜¨', 'ð˜´', 'á»¹', 'Ì§',
       '\uf06c', '\uf045', '\uf0ef', '\uf0c6', '\uf050', '\uf02a', 'â—Ž',
       'â†“', 'â–', 'â‰¥', 'â‚±', 'â—„', 'å£', 'ï¼…', 'ð”', 'â‚¬', 'â£', 'ð‘¦', 'ð‘’', 'â„',
       'âžº', 'âœ˜', 'ð‘ª', 'Ã¼', 'ðŸ…»', 'â€', 'ð¡', '\uf69a', '\uf4aa', '\ufeff',
       'á´„', 'â™«', 'â™¬', 'áºµ', 'âŽ•', 'â–¸', 'âˆ‡', 'ï¾ž', 'ð´', 'â‰ˆ', 'ã€—', 'ã€–', 'â–¡',
       'â‚', 'ð•¤', 'â—‡', 'â„±', 'ð“¢', 'â’ˆ', 'â’‰', 'â’Š', 'â’‹', '\uf0f2', 'Â¿', 'Â«',
       'â‡›', 'â€’', 'ðŸ…º', 'âž²', 'â˜‡', '\uf046', '\uf0bf', 'â™•', 'ï¹']
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):

        ###### DEL wrong words   
        sentence = ' '.join('' if word in kk else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + ' '                    
    document = new_sentence   
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document
def clean_text(text):
    import re
    import emoji
    text_clean=str(text).lower()
    if "thÃ´ng tin sáº£n pháº©m\n" in text_clean:
        text_clean=text_clean[text_clean.index("thÃ´ng tin sáº£n pháº©m\n"):]
    elif "mÃ´ táº£ sáº£n pháº©m\n" in text_clean:
        text_clean=text_clean[text_clean.index("mÃ´ táº£ sáº£n pháº©m\n"):]
    elif "\n\n" in text_clean:
        text_clean=text_clean[text_clean.index("\n\n"):]
    elif "\ngá»­i tá»«\n" in text_clean:
        text_clean=text_clean[text_clean.index("\ngá»­i tá»«\n")+len("\ngá»­i tá»«\n"):]
    
    # loáº¡i bá» pháº§n size
    text_clean=re.sub(r"\nsize[^\n]*","",text_clean)
    # loáº¡i bá» cÃ¡c hastag
    text_clean=re.sub(r"#[^#]*","",text_clean)
    # loáº¡i bá» cÃ¡c kÃ­ tá»± khÃ´ng há»£p lá»‡
    text_clean=re.sub(r"\n"," ",text_clean)
    text_clean=emoji.replace_emoji(text_clean)
    text_clean=re.sub('[\.\:\,\-\â€”\+\d\!\...\"\*\>\<\^\&\/\[\]\(\)\=\~\%]',' ',text_clean)
    # loáº¡i bá» cÃ¡c tá»« khÃ´ng cáº§n thiáº¿t
    text_clean=re.sub('\ss\s|\sm\s|\sl\s|\sxl|xxl|xxxl|xxxxl|2xl|3xl|4xl|size|\smm\s|\scm\s|\sm\s|\sg\s|\skg\s',' ',text_clean)
    text_clean=re.sub('\s+',' ',text_clean) 
    # ...
    return text_clean
# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»ŽÃ•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»žá» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»Ž|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»ž|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic
 
# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def covert_unicode(txt):
    import regex
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)
def process_special_word(text):
    new_text = ''
    text_lst = text.split()
    i= 0
    if 'khÃ´ng' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            #print(word)
            #print(i)
            if  word == 'khÃ´ng':
                next_idx = i+1
                if next_idx <= len(text_lst) -1:
                    word = word +'_'+ text_lst[next_idx]
                i= next_idx + 1
            else:
                i = i+1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()
def process_postag_thesea(text):
    from underthesea import word_tokenize, pos_tag, sent_tokenize
    import regex
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.','')
        ###### POS tag
        #lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        #lst_word_type = ['A','AB','V','VB','VY','R','C']
        #lst_word_type = ['A','AB','V','VB','VY','R','M','C']
        sentence = ' '.join( word[0] for word in pos_tag(word_tokenize(sentence, format="text")))
        new_document = new_document + sentence + ' '
    ###### DEL excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document
def remove_stopword(text, stopwords):
    import regex
    ###### REMOVE stop words
    document = ' '.join('' if word in stopwords else word for word in text.split())
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document